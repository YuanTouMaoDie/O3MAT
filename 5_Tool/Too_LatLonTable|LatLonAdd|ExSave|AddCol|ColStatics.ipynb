{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # 初始化一个空字典，用于存储所有站点经纬度信息\n",
    "# all_site_latlon_dict = {}\n",
    "\n",
    "# # 遍历2001到2019年\n",
    "# for year in range(2001, 2020):\n",
    "#     # 构建文件路径\n",
    "#     file_path = f'/backupdata/data_EPA/EQUATES/EQUATES_data/ds.input.aqs.o3.{year}.csv'\n",
    "#     try:\n",
    "#         # 读取CSV文件\n",
    "#         df = pd.read_csv(file_path)\n",
    "#         # 提取唯一站点及其经纬度\n",
    "#         unique_sites = df.drop_duplicates(subset='Site')[['Site', 'Lat', 'Lon']]\n",
    "#         # 将站点ID转换为字符串类型\n",
    "#         unique_sites['Site'] = unique_sites['Site'].astype(str)\n",
    "#         # 遍历每个唯一站点，更新总字典\n",
    "#         for _, row in unique_sites.iterrows():\n",
    "#             site = row['Site']\n",
    "#             lat = row['Lat']\n",
    "#             lon = row['Lon']\n",
    "#             all_site_latlon_dict[site] = {'Lat': lat, 'Lon': lon}\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"文件 {file_path} 未找到。\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"读取文件 {file_path} 时出现错误: {e}\")\n",
    "\n",
    "# # 处理新加入的文件\n",
    "# new_files = [\n",
    "#     '/backupdata/data_EPA/EQUATES/EQUATES_data/SMAT_OZONE_MDA1_APRSEP_STD70_2000_2022.CSV',\n",
    "#     '/backupdata/data_EPA/EQUATES/EQUATES_data/SMAT_OZONE_MDA8_MAYSEP_STD70_2000_2022.CSV'\n",
    "# ]\n",
    "\n",
    "# for file_path in new_files:\n",
    "#     try:\n",
    "#         df = pd.read_csv(file_path)\n",
    "#         # 假设新文件里的站点和经纬度列名和之前的一样，如果不同需修改\n",
    "#         unique_sites = df.drop_duplicates(subset='Site')[['Site', 'Lat', 'Lon']]\n",
    "#         unique_sites['Site'] = unique_sites['Site'].astype(str)\n",
    "#         for _, row in unique_sites.iterrows():\n",
    "#             site = row['Site']\n",
    "#             lat = row['Lat']\n",
    "#             lon = row['Lon']\n",
    "#             all_site_latlon_dict[site] = {'Lat': lat, 'Lon': lon}\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"文件 {file_path} 未找到。\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"读取文件 {file_path} 时出现错误: {e}\")\n",
    "\n",
    "# # 处理 MonitorsTimeRegion_Filter.csv 文件\n",
    "# monitors_file = '/output/Region/MonitorsTimeRegion_Filter.csv'\n",
    "# try:\n",
    "#     df = pd.read_csv(monitors_file)\n",
    "#     if 'site_id' in df.columns and 'Lat' in df.columns and 'Lon' in df.columns:\n",
    "#         unique_sites = df.drop_duplicates(subset='site_id')[['site_id', 'Lat', 'Lon']]\n",
    "#         unique_sites['site_id'] = unique_sites['site_id'].astype(str)\n",
    "#         for _, row in unique_sites.iterrows():\n",
    "#             site = row['site_id']\n",
    "#             lat = row['Lat']\n",
    "#             lon = row['Lon']\n",
    "#             all_site_latlon_dict[site] = {'Lat': lat, 'Lon': lon}\n",
    "#     else:\n",
    "#         print(f\"文件 {monitors_file} 中缺少必要的列（site_id, Lat, Lon）。\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"文件 {monitors_file} 未找到。\")\n",
    "# except Exception as e:\n",
    "#     print(f\"读取文件 {monitors_file} 时出现错误: {e}\")\n",
    "\n",
    "# # 将汇总的字典转换为DataFrame\n",
    "# result_df = pd.DataFrame.from_dict(all_site_latlon_dict, orient='index')\n",
    "# result_df.index.name = 'Site'\n",
    "\n",
    "# # 保存结果到CSV文件\n",
    "# output_path = '/backupdata/data_EPA/EQUATES/EQUATES_data/SitesTable2001-2020.csv'\n",
    "# try:\n",
    "#     result_df.to_csv(output_path)\n",
    "#     print(f\"结果已成功保存到 {output_path}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"保存文件时出现错误: {e}\")\n",
    "\n",
    "# # 输出结果数据表中唯一站点的信息\n",
    "# unique_sites = result_df.index.unique()\n",
    "# print(\"输出数据表中的唯一站点:\")\n",
    "# print(unique_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# # 读取第一个文件，提取 UniqueSite 和对应的 LatLon\n",
    "# file_path_1 = '/backupdata/data_EPA/EQUATES/EQUATES_data/SitesTable2001-2020.csv'\n",
    "# df_1 = pd.read_csv(file_path_1)\n",
    "# unique_sites = df_1.drop_duplicates(subset='Site')[['Site', 'Lat', 'Lon']]\n",
    "# unique_sites['Site'] = unique_sites['Site'].astype(str)\n",
    "# # 过滤掉带字母的站点\n",
    "# valid_sites = unique_sites[~unique_sites['Site'].str.contains(r'[a-zA-Z]', regex=True)]\n",
    "# site_latlon_dict = valid_sites.set_index('Site')[['Lat', 'Lon']].to_dict(orient='index')\n",
    "# # 找出被过滤掉的站点\n",
    "# filtered_out_sites_1 = set(unique_sites['Site']) - set(valid_sites['Site'])\n",
    "# print(f\"从站点表中过滤掉的站点: {filtered_out_sites_1}\")\n",
    "\n",
    "# # 站点表\n",
    "# unique_site_count = valid_sites['Site'].nunique()\n",
    "\n",
    "# # 读取第二个文件，只读取需要的列\n",
    "# file_path_2 = '/backupdata/data_EPA/aq_obs/routine/2011/AQS_hourly_data_2011.csv'\n",
    "# try:\n",
    "#     df_2 = pd.read_csv(file_path_2, usecols=['site_id', 'POCode', 'dateon', 'O3'])\n",
    "# except Exception as e:\n",
    "#     print(f\"读取文件时出现错误: {e}\")\n",
    "\n",
    "# filtered_df = df_2[(df_2['O3'] != -999)]\n",
    "# filtered_df['site_id'] = filtered_df['site_id'].astype(str)\n",
    "# # 过滤掉带字母的站点\n",
    "# valid_input_sites = filtered_df[~filtered_df['site_id'].str.contains(r'[a-zA-Z]', regex=True)]\n",
    "# # 找出被过滤掉的站点\n",
    "# filtered_out_sites_2 = set(filtered_df['site_id']) - set(valid_input_sites['site_id'])\n",
    "# print(f\"从输入文件中过滤掉的站点: {filtered_out_sites_2}\")\n",
    "# # 统计输入文件中的唯一站点个数\n",
    "# input_unique_site_count = valid_input_sites['site_id'].nunique()\n",
    "\n",
    "# # 添加经纬度信息\n",
    "# valid_input_sites['Lat'] = valid_input_sites['site_id'].map(lambda x: site_latlon_dict.get(x, {}).get('Lat'))\n",
    "# valid_input_sites['Lon'] = valid_input_sites['site_id'].map(lambda x: site_latlon_dict.get(x, {}).get('Lon'))\n",
    "\n",
    "# # 剔除不能添加上经纬度的行\n",
    "# valid_input_sites = valid_input_sites.dropna(subset=['Lat', 'Lon'])\n",
    "\n",
    "# # 统计输出文件中的唯一站点个数\n",
    "# output_unique_site_count = valid_input_sites['site_id'].nunique()\n",
    "\n",
    "# # 输出结果到指定文件\n",
    "# output_path = '/backupdata/data_EPA/aq_obs/routine/2011/AQS_hourly_data_2011_LatLon.csv'\n",
    "# valid_input_sites.to_csv(output_path, index=False)\n",
    "\n",
    "# print(f\"站点表中的唯一站点个数: {unique_site_count}\")\n",
    "# print(f\"输入文件中O3站点的数据行数: {input_unique_site_count}\")\n",
    "# print(f\"输出文件中O3站点个数: {output_unique_site_count}\")\n",
    "# print(f\"结果已成功保存到 {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # 定义输入和输出文件路径\n",
    "# input_file = '/DeepLearning/mnt/shixiansheng/data_fusion/output/W126/2011_W126_ST.csv'\n",
    "# output_file = '/DeepLearning/mnt/shixiansheng/data_fusion/output/W126/2011_Model_W126_ST.csv'\n",
    "\n",
    "# try:\n",
    "#     # 读取 CSV 文件\n",
    "#     df = pd.read_csv(input_file)\n",
    "\n",
    "#     # 提取所需的列\n",
    "#     selected_columns = df[['ROW', 'COL', 'model', 'Period']]\n",
    "\n",
    "#     # 将提取的列保存为新的 CSV 文件\n",
    "#     selected_columns.to_csv(output_file, index=False)\n",
    "#     print(f\"已成功提取指定列并保存到 {output_file}\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"错误: 未找到文件 {input_file}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"发生未知错误: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已成功添加 'Period' 列到 CSV 文件。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取 CSV 文件\n",
    "file_path = '/DeepLearning/mnt/shixiansheng/data_fusion/output/W126/2011_W126_ST_AtF_True.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 添加新列\n",
    "df['Period'] = 'W126'\n",
    "\n",
    "# 保存修改后的数据到原文件\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"已成功添加 'Period' 列到 CSV 文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# file_path = '/DeepLearning/mnt/shixiansheng/data_fusion/output/W126/2004_Monitor_W126.csv'\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# count_site_id = df['site_id'].count()\n",
    "# count_O3 = df['O3'].count()\n",
    "# count_Period = df['Period'].count()\n",
    "# print(f\"site_id的非空值数量: {count_site_id}\")\n",
    "# print(f\"O3的非空值数量: {count_O3}\")\n",
    "# print(f\"Period的非空值数量: {count_Period}\")\n",
    "\n",
    "# mean_O3 = df['O3'].mean()\n",
    "# print(f\"O3的均值: {mean_O3}\")\n",
    "\n",
    "# std_O3 = df['O3'].std()\n",
    "# print(f\"O3的标准差: {std_O3}\")\n",
    "\n",
    "# min_O3 = df['O3'].min()\n",
    "# print(f\"O3的最小值: {min_O3}\")\n",
    "\n",
    "# q25_O3 = df['O3'].quantile(0.25)\n",
    "# print(f\"O3的25%分位数: {q25_O3}\")\n",
    "\n",
    "# median_O3 = df['O3'].median()\n",
    "# print(f\"O3的中位数: {median_O3}\")\n",
    "\n",
    "# q75_O3 = df['O3'].quantile(0.75)\n",
    "# print(f\"O3的75%分位数: {q75_O3}\")\n",
    "\n",
    "# max_O3 = df['O3'].max()\n",
    "# print(f\"O3的最大值: {max_O3}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
