{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# HourlyFullCheck\n",
    "# file_path = '/backupdata/data_EPA/aq_obs/routine/2011/AQS_hourly_data_2011_LatLon.csv'\n",
    "# complete_count = 0  # 用于统计完整数据的数量\n",
    "# incomplete_count = 0  # 用于统计不完整数据的数量\n",
    "# try:\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     # 将 dateon 列转换为日期时间类型\n",
    "#     df['dateon'] = pd.to_datetime(df['dateon'])\n",
    "\n",
    "#     # 提取日期和小时信息\n",
    "#     df['date'] = df['dateon'].dt.date\n",
    "#     df['hour'] = df['dateon'].dt.hour\n",
    "\n",
    "#     # 按站点 ID 和日期分组\n",
    "#     grouped = df.groupby(['site_id', 'date'])\n",
    "\n",
    "#     # 检查每组是否有 24 个小时的数据\n",
    "#     for (site_id, date), group in grouped:\n",
    "#         if len(group['hour'].unique()) == 24:\n",
    "#             complete_count += 1\n",
    "#         else:\n",
    "#             incomplete_count += 1\n",
    "\n",
    "#     print(f\"有完整 24 小时数据的站点-日期组合数量: {complete_count}\")\n",
    "#     print(f\"数据不完整的站点-日期组合数量: {incomplete_count}\")\n",
    "\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"错误: 文件 {file_path} 未找到。\")\n",
    "# except Exception as e:\n",
    "#     print(f\"发生未知错误: {e}\")\n",
    "# from itertools import product\n",
    "\n",
    "# #2011年输出\n",
    "# # 有完整 24 小时数据的站点-日期组合数量: 246920\n",
    "# # 数据不完整的站点-日期组合数量: 136149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # CalMissingHour\n",
    "# def print_unique_sites_count():\n",
    "#     try:\n",
    "#         # 读取输入文件\n",
    "#         input_file_path = '/backupdata/data_EPA/aq_obs/routine/2011/AQS_hourly_data_2011_LatLon.csv'\n",
    "#         input_df = pd.read_csv(input_file_path)\n",
    "#         input_df['dateon'] = pd.to_datetime(input_df['dateon'])\n",
    "\n",
    "#         # 获取所有唯一的站点信息\n",
    "#         all_sites = input_df.groupby('site_id').first()[['Lat', 'Lon']]\n",
    "\n",
    "#         periods = {\n",
    "#             'DJF': pd.date_range('2011-01-01 00:00:00', '2011-02-28 23:00:00', freq='H').union(\n",
    "#                 pd.date_range('2011-12-01 00:00:00', '2011-12-31 23:00:00', freq='H')),\n",
    "#             'MAM': pd.date_range('2011-03-01 00:00:00', '2011-05-31 23:00:00', freq='H'),\n",
    "#             'JJA': pd.date_range('2011-06-01 00:00:00', '2011-08-31 23:00:00', freq='H'),\n",
    "#             'SON': pd.date_range('2011-09-01 00:00:00', '2011-11-30 23:00:00', freq='H'),\n",
    "#             'Apr - Sep': pd.date_range('2011-04-01 00:00:00', '2011-09-30 23:00:00', freq='H'),\n",
    "#             'Annual': pd.date_range('2011-01-01 00:00:00', '2011-12-31 23:00:00', freq='H'),\n",
    "#         }\n",
    "\n",
    "#         results = []\n",
    "#         for period_name, period_dates in periods.items():\n",
    "#             all_hours_count = len(period_dates)\n",
    "#             # 按站点分组，使用向量化操作计算每个站点在该周期内的现有小时数\n",
    "#             period_df = input_df[input_df['dateon'].isin(period_dates)]\n",
    "#             if not period_df.empty:\n",
    "#                 site_grouped = period_df.groupby('site_id')\n",
    "#                 existing_hours = site_grouped['dateon'].nunique()\n",
    "#             else:\n",
    "#                 existing_hours = pd.Series([0] * len(all_sites), index=all_sites.index)\n",
    "\n",
    "#             # 计算所有站点的缺失小时数\n",
    "#             missing_hours = all_hours_count - existing_hours.reindex(all_sites.index, fill_value=0)\n",
    "#             # 创建该周期的结果 DataFrame\n",
    "#             period_result = pd.DataFrame({\n",
    "#                 'Period': period_name,\n",
    "#                 'site_id': all_sites.index,\n",
    "#                 'Lat': all_sites['Lat'],\n",
    "#                 'Lon': all_sites['Lon'],\n",
    "#                 'MissingHours': missing_hours\n",
    "#             })\n",
    "#             results.append(period_result)\n",
    "\n",
    "#             # 打印该周期的唯一站点数量\n",
    "#             unique_sites_count = len(period_result['site_id'].unique())\n",
    "#             print(f\"周期 {period_name} 的唯一站点数量: {unique_sites_count}\")\n",
    "\n",
    "#         # 合并所有周期的结果\n",
    "#         if results:\n",
    "#             result_df = pd.concat(results, ignore_index=True)\n",
    "#             # 保存结果到输出文件\n",
    "#             output_file_path = '/backupdata/data_EPA/aq_obs/routine/2011/AQS_hourly_data_2011_LatLon_MissingHours.csv'\n",
    "#             result_df.to_csv(output_file_path, index=False)\n",
    "#             print(f\"结果已保存到 {output_file_path}\")\n",
    "\n",
    "#             # 打印每个周期缺失最大的站点\n",
    "#             for period in result_df['Period'].unique():\n",
    "#                 period_subset = result_df[result_df['Period'] == period]\n",
    "#                 max_missing = period_subset[period_subset['MissingHours'] == period_subset['MissingHours'].max()]\n",
    "#                 print(f\"周期 {period} 中缺失小时数最大的站点信息：\")\n",
    "#                 print(max_missing)\n",
    "#         else:\n",
    "#             print(\"没有符合条件的数据，未生成结果文件。\")\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"错误：未找到输入文件 {input_file_path}。\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"发生未知错误：{e}\")\n",
    "\n",
    "\n",
    "# print_unique_sites_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "#Day\n",
    "# # 加载数据\n",
    "# file_path = \"/backupdata/data_EPA/EQUATES/EQUATES_data/ds.input.aqs.o3.2011.csv\"\n",
    "# data = pd.read_csv(file_path)\n",
    "\n",
    "# # 将 'Date' 列转换为日期时间格式\n",
    "# data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# # 定义季度函数\n",
    "# def get_quarter(month):\n",
    "#     if month in [12, 1, 2]:\n",
    "#         return 'DFJ'\n",
    "#     elif month in [3, 4, 5]:\n",
    "#         return 'MAM'\n",
    "#     elif month in [6, 7, 8]:\n",
    "#         return 'JJA'\n",
    "#     elif month in [9, 10, 11]:\n",
    "#         return 'SON'\n",
    "\n",
    "# # 添加季度列\n",
    "# data['Quarter'] = data['Date'].dt.month.apply(get_quarter)\n",
    "\n",
    "# # 生成每个季度的日期范围\n",
    "# quarters = {\n",
    "#     'DJF': pd.date_range('2011-01-01', '2011-02-28').union(pd.date_range('2011-12-01', '2011-12-31')),\n",
    "#     'MAM': pd.date_range('2011-03-01', '2011-05-31'),\n",
    "#     'JJA': pd.date_range('2011-06-01', '2011-08-31'),\n",
    "#     'SON': pd.date_range('2011-09-01', '2011-11-30'),\n",
    "#     'Apr-Sep': pd.date_range('2011-04-01', '2011-09-30'),\n",
    "#     'Annual': pd.date_range('2011-01-01', '2011-12-31'),\n",
    "# }\n",
    "\n",
    "# # 计算缺失日期的数量\n",
    "# def count_missing_dates(site_data, quarter_dates):\n",
    "#     site_quarter_data = site_data[site_data['Date'].isin(quarter_dates)]\n",
    "#     missing_dates = len(quarter_dates) - len(site_quarter_data)\n",
    "#     return missing_dates\n",
    "\n",
    "# # 统计每个站点各个季度的缺失天数\n",
    "# missing_days_per_site = []\n",
    "\n",
    "# for site in data['Site'].unique():\n",
    "#     site_data = data[data['Site'] == site]\n",
    "#     lat = site_data['Lat'].iloc[0]  # 获取站点纬度\n",
    "#     lon = site_data['Lon'].iloc[0]  # 获取站点经度\n",
    "\n",
    "#     for quarter, quarter_dates in quarters.items():\n",
    "#         missing_days = count_missing_dates(site_data, quarter_dates)\n",
    "#         missing_days_per_site.append([site, lon, lat, missing_days, quarter])\n",
    "\n",
    "# # 创建 DataFrame\n",
    "# missing_days_df = pd.DataFrame(missing_days_per_site, columns=['StationID', 'Lon', 'Lat', 'MissingDays', 'Period'])\n",
    "\n",
    "# # ...existing code...\n",
    "\n",
    "# # 保存结果到 CSV 文件\n",
    "# output_file_path = \"/backupdata/data_EPA/EQUATES/EQUATES_data/ds.input.aqs.o3.2011_MissingDays.csv\"\n",
    "# missing_days_df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # DiffForDayAndHour\n",
    "# # 定义时间段\n",
    "# periods = {\n",
    "#     'DJF': pd.date_range('2011-01-01', '2011-02-28').union(pd.date_range('2011-12-01', '2011-12-31')),\n",
    "#     'MAM': pd.date_range('2011-03-01', '2011-05-31'),\n",
    "#     'JJA': pd.date_range('2011-06-01', '2011-08-31'),\n",
    "#     'SON': pd.date_range('2011-09-01', '2011-11-30'),\n",
    "#     'Apr-Sep': pd.date_range('2011-04-01', '2011-09-30'),\n",
    "#     'Annual': pd.date_range('2011-01-01', '2011-12-31'),\n",
    "# }\n",
    "\n",
    "# # 读取小时数据表和日数据表\n",
    "# try:\n",
    "#     hourly_data = pd.read_csv('/backupdata/data_EPA/aq_obs/routine/2011/AQS_hourly_data_2011_LatLon.csv')\n",
    "#     daily_data = pd.read_csv('/backupdata/data_EPA/EQUATES/EQUATES_data/ds.input.aqs.o3.2011.csv')\n",
    "\n",
    "#     # 将小时数据的 'dateon' 列和日数据的 'Date' 列转换为 datetime 类型\n",
    "#     hourly_data['dateon'] = pd.to_datetime(hourly_data['dateon'])\n",
    "#     daily_data['Date'] = pd.to_datetime(daily_data['Date'])\n",
    "\n",
    "#     result = []\n",
    "\n",
    "#     for period_name, date_range in periods.items():\n",
    "#         # 筛选出该时间段内的小时数据和日数据\n",
    "#         hourly_period_data_isin = hourly_data[hourly_data['dateon'].isin(date_range)]\n",
    "#         daily_period_data_isin = daily_data[daily_data['Date'].isin(date_range)]\n",
    "\n",
    "#         hourly_period_data_logic = hourly_data[(hourly_data['dateon'] >= date_range.min()) & (hourly_data['dateon'] <= date_range.max())]\n",
    "#         daily_period_data_logic = daily_data[(daily_data['Date'] >= date_range.min()) & (daily_data['Date'] <= date_range.max())]\n",
    "\n",
    "#         # 统计每个站点在 isin 筛选后的记录数量\n",
    "#         hourly_site_counts_isin = hourly_period_data_isin.groupby('site_id').size()\n",
    "#         daily_site_counts_isin = daily_period_data_isin.groupby('Site').size()\n",
    "\n",
    "#         # 统计每个站点在逻辑判断筛选后的记录数量\n",
    "#         hourly_site_counts_logic = hourly_period_data_logic.groupby('site_id').size()\n",
    "#         daily_site_counts_logic = daily_period_data_logic.groupby('Site').size()\n",
    "\n",
    "#         print(f\"时间段 {period_name}：\")\n",
    "#         print(\"使用 isin 筛选 - 小时数据各站点记录数量统计：\")\n",
    "#         print(hourly_site_counts_isin)\n",
    "#         print(\"使用 isin 筛选 - 日数据各站点记录数量统计：\")\n",
    "#         print(daily_site_counts_isin)\n",
    "\n",
    "#         print(\"使用逻辑判断筛选 - 小时数据各站点记录数量统计：\")\n",
    "#         print(hourly_site_counts_logic)\n",
    "#         print(\"使用逻辑判断筛选 - 日数据各站点记录数量统计：\")\n",
    "#         print(daily_site_counts_logic)\n",
    "\n",
    "#         # 找出该时间段内小时数据和日数据中站点 ID 的差异\n",
    "#         diff_site_ids = set(hourly_period_data_isin['site_id']).symmetric_difference(set(daily_period_data_isin['Site']))\n",
    "\n",
    "#         # 获取差异站点 ID 的 Lat 和 Lon 信息\n",
    "#         for site_id in diff_site_ids:\n",
    "#             site_info = hourly_period_data_isin[hourly_period_data_isin['site_id'] == site_id].head(1)[['site_id', 'Lat', 'Lon']]\n",
    "#             if not site_info.empty:\n",
    "#                 site_info['Period'] = period_name\n",
    "#                 site_info['Flag'] = 2\n",
    "#             else:\n",
    "#                 site_info = daily_period_data_isin[daily_period_data_isin['Site'] == site_id].head(1)[['Site', 'Lat', 'Lon']]\n",
    "#                 site_info.rename(columns={'Site': 'site_id'}, inplace=True)\n",
    "#                 site_info['Period'] = period_name\n",
    "#                 site_info['Flag'] = 1\n",
    "#             result.append(site_info)\n",
    "\n",
    "#     # 合并结果\n",
    "#     result_df = pd.concat(result, ignore_index=True)\n",
    "\n",
    "#     # 输出结果到指定路径\n",
    "#     output_path = '/backupdata/data_EPA/aq_obs/routine/2011/2011_site_id_difference.csv'\n",
    "#     result_df.to_csv(output_path, index=False)\n",
    "#     print(f\"结果已保存到 {output_path}\")\n",
    "\n",
    "# except FileNotFoundError:\n",
    "#     print(\"文件未找到，请检查文件路径是否正确。\")\n",
    "# except Exception as e:\n",
    "#     print(f\"发生未知错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # 定义时间段\n",
    "# periods = {\n",
    "#     'DJF': pd.date_range('2011-01-01', '2011-02-28').union(pd.date_range('2011-12-01', '2011-12-31')),\n",
    "#     'MAM': pd.date_range('2011-03-01', '2011-05-31'),\n",
    "#     'JJA': pd.date_range('2011-06-01', '2011-08-31'),\n",
    "#     'SON': pd.date_range('2011-09-01', '2011-11-30'),\n",
    "#     'Apr-Sep': pd.date_range('2011-04-01', '2011-09-30'),\n",
    "#     'Annual': pd.date_range('2011-01-01', '2011-12-31'),\n",
    "# }\n",
    "\n",
    "# # 读取小时数据表和日数据表\n",
    "# try:\n",
    "#     hourly_data = pd.read_csv('/backupdata/data_EPA/aq_obs/routine/2011/AQS_hourly_data_2011_LatLon.csv')\n",
    "#     daily_data = pd.read_csv('/backupdata/data_EPA/EQUATES/EQUATES_data/ds.input.aqs.o3.2011.csv')\n",
    "\n",
    "#     # 将小时数据的 'dateon' 列和日数据的 'Date' 列转换为 datetime 类型\n",
    "#     hourly_data['dateon'] = pd.to_datetime(hourly_data['dateon'])\n",
    "#     daily_data['Date'] = pd.to_datetime(daily_data['Date'])\n",
    "\n",
    "#     result = []\n",
    "\n",
    "#     for period_name, date_range in periods.items():\n",
    "#         if period_name == 'DJF':\n",
    "#             # 对于 DJF 时间段，分别处理两段日期范围\n",
    "#             first_range = pd.date_range('2011-01-01', '2011-02-28')\n",
    "#             second_range = pd.date_range('2011-12-01', '2011-12-31')\n",
    "#             hourly_period_data = hourly_data[\n",
    "#                 ((hourly_data['dateon'] >= first_range.min()) & (hourly_data['dateon'] <= first_range.max())) |\n",
    "#                 ((hourly_data['dateon'] >= second_range.min()) & (hourly_data['dateon'] <= second_range.max()))\n",
    "#             ]\n",
    "#             daily_period_data = daily_data[\n",
    "#                 ((daily_data['Date'] >= first_range.min()) & (daily_data['Date'] <= first_range.max())) |\n",
    "#                 ((daily_data['Date'] >= second_range.min()) & (daily_data['Date'] <= second_range.max()))\n",
    "#             ]\n",
    "#         else:\n",
    "#             # 其他时间段使用常规逻辑判断筛选\n",
    "#             hourly_period_data = hourly_data[(hourly_data['dateon'] >= date_range.min()) & (hourly_data['dateon'] <= date_range.max())]\n",
    "#             daily_period_data = daily_data[(daily_data['Date'] >= date_range.min()) & (daily_data['Date'] <= date_range.max())]\n",
    "\n",
    "#         # 计算各个时间段各自的 Unique 站点个数\n",
    "#         hourly_unique_count = len(set(hourly_period_data['site_id']))\n",
    "#         daily_unique_count = len(set(daily_period_data['Site']))\n",
    "\n",
    "#         print(f\"时间段 {period_name}：\")\n",
    "#         print(f\"小时数据的 Unique 站点个数: {hourly_unique_count}\")\n",
    "#         print(f\"日数据的 Unique 站点个数: {daily_unique_count}\")\n",
    "\n",
    "#         # 找出该时间段内小时数据和日数据中站点 ID 的差异\n",
    "#         diff_site_ids = set(hourly_period_data['site_id']).symmetric_difference(set(daily_period_data['Site']))\n",
    "\n",
    "#         # 获取差异站点 ID 的 Lat 和 Lon 信息\n",
    "#         for site_id in diff_site_ids:\n",
    "#             site_info = hourly_period_data[hourly_period_data['site_id'] == site_id].head(1)[['site_id', 'Lat', 'Lon']]\n",
    "#             if not site_info.empty:\n",
    "#                 site_info['Period'] = period_name\n",
    "#                 site_info['Flag'] = 2\n",
    "#             else:\n",
    "#                 site_info = daily_period_data[daily_period_data['Site'] == site_id].head(1)[['Site', 'Lat', 'Lon']]\n",
    "#                 site_info.rename(columns={'Site': 'site_id'}, inplace=True)\n",
    "#                 site_info['Period'] = period_name\n",
    "#                 site_info['Flag'] = 1\n",
    "#             result.append(site_info)\n",
    "\n",
    "#     # 合并结果\n",
    "#     result_df = pd.concat(result, ignore_index=True)\n",
    "\n",
    "#     # 输出结果到指定路径\n",
    "#     output_path = '/backupdata/data_EPA/aq_obs/routine/2011/2011_site_id_difference.csv'\n",
    "#     result_df.to_csv(output_path, index=False)\n",
    "#     print(f\"结果已保存到 {output_path}\")\n",
    "\n",
    "# except FileNotFoundError:\n",
    "#     print(\"文件未找到，请检查文件路径是否正确。\")\n",
    "# except Exception as e:\n",
    "#     print(f\"发生未知错误: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剔除了 3 个站点。\n",
      "剔除后原数据表还剩 1243 个站点。\n",
      "           site_id  POCode               dateon  O3        Lat         Lon\n",
      "0         10030010       1  2012-03-01 01:00:00  18  30.497478  -87.880258\n",
      "1         10030010       1  2012-03-01 02:00:00  17  30.497478  -87.880258\n",
      "2         10030010       1  2012-03-01 03:00:00  16  30.497478  -87.880258\n",
      "3         10030010       1  2012-03-01 04:00:00  15  30.497478  -87.880258\n",
      "4         10030010       1  2012-03-01 05:00:00  15  30.497478  -87.880258\n",
      "...            ...     ...                  ...  ..        ...         ...\n",
      "8491320  530110011       1  2012-09-30 19:00:00  28  45.616667 -122.516667\n",
      "8491321  530110011       1  2012-09-30 20:00:00  25  45.616667 -122.516667\n",
      "8491322  530110011       1  2012-09-30 21:00:00  21  45.616667 -122.516667\n",
      "8491323  530110011       1  2012-09-30 22:00:00  14  45.616667 -122.516667\n",
      "8491324  530110011       1  2012-09-30 23:00:00   8  45.616667 -122.516667\n",
      "\n",
      "[8483113 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取AQS_hourly_data_2011_LatLon.csv文件\n",
    "aqs_data = pd.read_csv('/backupdata/data_EPA/aq_obs/routine/2012/AQS_hourly_data_2012_LatLon.csv')\n",
    "# aqs_data = pd.read_csv('/backupdata/data_EPA/EQUATES/EQUATES_data/ds.input.aqs.o3.2011.csv')\n",
    "\n",
    "# 读取2011_site_id_difference.csv文件\n",
    "difference_data = pd.read_csv('/backupdata/data_EPA/aq_obs/routine/2011/2011_site_id_difference.csv')\n",
    "\n",
    "# 筛选出Period为Annual，Flag为2的行\n",
    "to_remove = difference_data[(difference_data['Period'] == 'Annual') & (difference_data['Flag'] == 1)]['site_id']\n",
    "\n",
    "# 剔除对应ID的行\n",
    "remaining_data = aqs_data[~aqs_data['site_id'].isin(to_remove)]\n",
    "\n",
    "# 计算剔除的站点数和剩余的站点数\n",
    "# 对site_id列去重后统计数量\n",
    "removed_count = len(to_remove.unique())\n",
    "remaining_count = len(remaining_data['site_id'].unique())\n",
    "\n",
    "# 输出剔除的站点数和剩余的站点数\n",
    "print(f\"剔除了 {removed_count} 个站点。\")\n",
    "print(f\"剔除后原数据表还剩 {remaining_count} 个站点。\")\n",
    "\n",
    "# 输出剔除后的数据表\n",
    "print(remaining_data)\n",
    "\n",
    "# 保存剔除后的数据表\n",
    "remaining_data.to_csv('/backupdata/data_EPA/aq_obs/routine/2012/AQS_hourly_data_2012_LatLon_Exsame.csv', index=False)\n",
    "# remaining_data.to_csv('/backupdata/data_EPA/EQUATES/EQUATES_data/ds.input.aqs.o3.2010_Exsame.csv')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
