{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# #提取出开头8的站点\n",
    "# def extract_site_latlon(csv_file, output_csv):\n",
    "#     try:\n",
    "#         # 读取 CSV 文件\n",
    "#         df = pd.read_csv(csv_file)\n",
    "\n",
    "#         # 筛选 Site 为 8 位数字且开头为 8 的行\n",
    "#         filtered_df = df[df['Site'].astype(str).str.match(r'^8\\d{8}$')]\n",
    "\n",
    "#         # 提取 Site、Lat 和 Lon 列\n",
    "#         result_df = filtered_df[['Site', 'Lat', 'Lon']]\n",
    "\n",
    "#         # 保存到 CSV 文件\n",
    "#         result_df.to_csv(output_csv, index=False)\n",
    "#         print(f\"数据已成功保存到 {output_csv}\")\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"错误: 文件未找到，请检查文件路径。\")\n",
    "#     except KeyError:\n",
    "#         print(\"错误: 数据表中没有 'Site'、'Lat' 或 'Lon' 列。\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"发生未知错误: {e}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     csv_file = '/backupdata/data_EPA/EQUATES/EQUATES_data/SitesTable2001-2020.csv'\n",
    "#     output_csv = '/backupdata/data_EPA/EQUATES/EQUATES_data/SitesTable2001-2020_8.csv'\n",
    "#     extract_site_latlon(csv_file, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #画站点图\n",
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from shapely.geometry import Point\n",
    "\n",
    "# # 读取 GeoJSON 文件\n",
    "# us_map = gpd.read_file('output/Region/USA_State.json')\n",
    "\n",
    "# # 筛选出美国本土州的数据，排除阿拉斯加和夏威夷\n",
    "# us_continental_map = us_map[~us_map['name'].isin(['Alaska', 'Hawaii'])]\n",
    "\n",
    "# # 读取包含数据点的 CSV 文件\n",
    "# data = pd.read_csv('/backupdata/data_EPA/EQUATES/EQUATES_data/SitesTable2001-2020_8.csv')\n",
    "\n",
    "# # 将数据点转换为 GeoDataFrame\n",
    "# points = gpd.GeoDataFrame(\n",
    "#     data,\n",
    "#     geometry=gpd.points_from_xy(data['Lon'], data['Lat']),\n",
    "#     crs=us_continental_map.crs\n",
    "# )\n",
    "\n",
    "# # 创建绘图对象\n",
    "# fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# # 绘制美国本土地图，设置填充颜色和边界颜色及样式\n",
    "# us_continental_map.plot(ax=ax, color='#f0f0f0', edgecolor='gray', linewidth=0.5)\n",
    "\n",
    "# # 在地图上绘制数据点，设置颜色、大小和透明度\n",
    "# points.plot(ax=ax, color='Crimson', markersize=6, alpha=0.7)\n",
    "\n",
    "# # 设置坐标轴范围以显示左下角区域，你可以根据实际情况调整\n",
    "# xlim = (-125, -65)\n",
    "# ylim = (20, 50)\n",
    "# ax.set_xlim(xlim)\n",
    "# ax.set_ylim(ylim)\n",
    "\n",
    "# # 添加标题\n",
    "# ax.set_title('US Continental Lower Left Region with Data Points', fontsize=16)\n",
    "\n",
    "# # 设置坐标轴标签\n",
    "# ax.set_xlabel('Longitude')\n",
    "# ax.set_ylabel('Latitude')\n",
    "\n",
    "# # 设置坐标轴刻度\n",
    "# xticks = range(-125, -65, 5)\n",
    "# yticks = range(20, 50, 1)\n",
    "# ax.set_xticks(xticks)\n",
    "# ax.set_yticks(yticks)\n",
    "\n",
    "# # 显示网格线\n",
    "# ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# # 显示图形\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #画出不同点不同时区\n",
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from shapely.geometry import Point\n",
    "# import matplotlib.cm as cm\n",
    "# import numpy as np\n",
    "\n",
    "# # 读取美国地图的 GeoJSON 文件\n",
    "# us_map = gpd.read_file('output/Region/USA_State.json')\n",
    "\n",
    "# # 筛选出美国本土州的数据，排除阿拉斯加和夏威夷\n",
    "# us_continental_map = us_map[~us_map['name'].isin(['Alaska', 'Hawaii'])]\n",
    "\n",
    "# # 进一步通过经纬度范围筛选（示例范围，可按需调整）\n",
    "# min_lon = -125\n",
    "# max_lon = -65\n",
    "# min_lat = 25\n",
    "# max_lat = 50\n",
    "# us_continental_map = us_continental_map.cx[min_lon:max_lon, min_lat:max_lat]\n",
    "\n",
    "# # 读取包含监测点信息的 CSV 文件\n",
    "# data = pd.read_csv('output/Region/MonitorsTimeRegion_Filter_ST_QA_Compare.csv')\n",
    "\n",
    "# # 过滤 Lat 大于 24 且小于 50 的数据\n",
    "# filtered_data = data[(data['Lat'] > 24) & (data['Lat'] < 50)]\n",
    "\n",
    "# # 将过滤后的数据点转换为 GeoDataFrame\n",
    "# points = gpd.GeoDataFrame(\n",
    "#     filtered_data,\n",
    "#     geometry=gpd.points_from_xy(filtered_data['Lon'], filtered_data['Lat']),\n",
    "#     crs=us_continental_map.crs\n",
    "# )\n",
    "\n",
    "# # 指定用于分组绘图的数据列名称\n",
    "# input_column = 'Fortan - gmt_offset'  # 请替换为实际的列名\n",
    "\n",
    "# # 指定分组值对应的颜色\n",
    "# color_mapping = {\n",
    "#     0: 'white',\n",
    "#     -1:'Red',\n",
    "#     1:'green',\n",
    "#     # 可以根据实际情况添加更多分组值和对应的颜色\n",
    "# }\n",
    "\n",
    "# # 根据用户指定的列分组\n",
    "# grouped = points.groupby(input_column)\n",
    "\n",
    "# # 创建绘图对象\n",
    "# fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# # 绘制美国本土地图\n",
    "# us_continental_map.plot(ax=ax, color='#f0f0f0', edgecolor='gray', linewidth=0.5)\n",
    "\n",
    "# # 遍历每个分组并绘制数据点\n",
    "# for group_value, group in grouped:\n",
    "#     color = color_mapping.get(group_value, 'gray')  # 如果未指定颜色，使用灰色\n",
    "#     group.plot(ax=ax, color=color, markersize=6, alpha=0.7, label=f'{input_column} {group_value}')\n",
    "\n",
    "# # 添加图例\n",
    "# ax.legend()\n",
    "\n",
    "# # 添加标题\n",
    "# ax.set_title(f'US Continental Map with Monitoring Points by {input_column}', fontsize=16)\n",
    "\n",
    "# # 设置坐标轴标签\n",
    "# ax.set_xlabel('Longitude')\n",
    "# ax.set_ylabel('Latitude')\n",
    "\n",
    "# # 显示图形\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # DiffForDayAndHour\n",
    "# # 定义时间段\n",
    "# periods = {\n",
    "#     'DJF': pd.date_range('2011-01-01', '2011-02-28').union(pd.date_range('2011-12-01', '2011-12-31')),\n",
    "#     'MAM': pd.date_range('2011-03-01', '2011-05-31'),\n",
    "#     'JJA': pd.date_range('2011-06-01', '2011-08-31'),\n",
    "#     'SON': pd.date_range('2011-09-01', '2011-11-30'),\n",
    "#     'Apr-Sep': pd.date_range('2011-04-01', '2011-09-30'),\n",
    "#     'Annual': pd.date_range('2011-01-01', '2011-12-31'),\n",
    "# }\n",
    "\n",
    "# # 读取小时数据表和日数据表\n",
    "# try:\n",
    "#     hourly_data = pd.read_csv('/backupdata/data_EPA/aq_obs/routine/2011/AQS_hourly_data_2011_LatLon.csv')\n",
    "#     daily_data = pd.read_csv('/backupdata/data_EPA/EQUATES/EQUATES_data/ds.input.aqs.o3.2011.csv')\n",
    "\n",
    "#     # 将小时数据的 'dateon' 列和日数据的 'Date' 列转换为 datetime 类型\n",
    "#     hourly_data['dateon'] = pd.to_datetime(hourly_data['dateon'])\n",
    "#     daily_data['Date'] = pd.to_datetime(daily_data['Date'])\n",
    "\n",
    "#     result = []\n",
    "\n",
    "#     for period_name, date_range in periods.items():\n",
    "#         # 筛选出该时间段内的小时数据和日数据\n",
    "#         hourly_period_data_isin = hourly_data[hourly_data['dateon'].isin(date_range)]\n",
    "#         daily_period_data_isin = daily_data[daily_data['Date'].isin(date_range)]\n",
    "\n",
    "#         hourly_period_data_logic = hourly_data[(hourly_data['dateon'] >= date_range.min()) & (hourly_data['dateon'] <= date_range.max())]\n",
    "#         daily_period_data_logic = daily_data[(daily_data['Date'] >= date_range.min()) & (daily_data['Date'] <= date_range.max())]\n",
    "\n",
    "#         # 统计每个站点在 isin 筛选后的记录数量\n",
    "#         hourly_site_counts_isin = hourly_period_data_isin.groupby('site_id').size()\n",
    "#         daily_site_counts_isin = daily_period_data_isin.groupby('Site').size()\n",
    "\n",
    "#         # 统计每个站点在逻辑判断筛选后的记录数量\n",
    "#         hourly_site_counts_logic = hourly_period_data_logic.groupby('site_id').size()\n",
    "#         daily_site_counts_logic = daily_period_data_logic.groupby('Site').size()\n",
    "\n",
    "#         print(f\"时间段 {period_name}：\")\n",
    "#         print(\"使用 isin 筛选 - 小时数据各站点记录数量统计：\")\n",
    "#         print(hourly_site_counts_isin)\n",
    "#         print(\"使用 isin 筛选 - 日数据各站点记录数量统计：\")\n",
    "#         print(daily_site_counts_isin)\n",
    "\n",
    "#         print(\"使用逻辑判断筛选 - 小时数据各站点记录数量统计：\")\n",
    "#         print(hourly_site_counts_logic)\n",
    "#         print(\"使用逻辑判断筛选 - 日数据各站点记录数量统计：\")\n",
    "#         print(daily_site_counts_logic)\n",
    "\n",
    "#         # 找出该时间段内小时数据和日数据中站点 ID 的差异\n",
    "#         diff_site_ids = set(hourly_period_data_isin['site_id']).symmetric_difference(set(daily_period_data_isin['Site']))\n",
    "\n",
    "#         # 获取差异站点 ID 的 Lat 和 Lon 信息\n",
    "#         for site_id in diff_site_ids:\n",
    "#             site_info = hourly_period_data_isin[hourly_period_data_isin['site_id'] == site_id].head(1)[['site_id', 'Lat', 'Lon']]\n",
    "#             if not site_info.empty:\n",
    "#                 site_info['Period'] = period_name\n",
    "#                 site_info['Flag'] = 2\n",
    "#             else:\n",
    "#                 site_info = daily_period_data_isin[daily_period_data_isin['Site'] == site_id].head(1)[['Site', 'Lat', 'Lon']]\n",
    "#                 site_info.rename(columns={'Site': 'site_id'}, inplace=True)\n",
    "#                 site_info['Period'] = period_name\n",
    "#                 site_info['Flag'] = 1\n",
    "#             result.append(site_info)\n",
    "\n",
    "#     # 合并结果\n",
    "#     result_df = pd.concat(result, ignore_index=True)\n",
    "\n",
    "#     # 输出结果到指定路径\n",
    "#     output_path = '/backupdata/data_EPA/aq_obs/routine/2011/2011_site_id_difference.csv'\n",
    "#     result_df.to_csv(output_path, index=False)\n",
    "#     print(f\"结果已保存到 {output_path}\")\n",
    "\n",
    "# except FileNotFoundError:\n",
    "#     print(\"文件未找到，请检查文件路径是否正确。\")\n",
    "# except Exception as e:\n",
    "#     print(f\"发生未知错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from shapely.geometry import Point\n",
    "# from scipy.spatial import cKDTree\n",
    "\n",
    "# # 读取包含监测点信息的 CSV 文件\n",
    "# data = pd.read_csv('output/Region/MonitorsTimeRegion_Filter.csv')\n",
    "\n",
    "# # 分离有 gmt_offset 和 epa_region 的点和没有这些信息的点\n",
    "# known_points = data.dropna(subset=['gmt_offset', 'epa_region'])\n",
    "# unknown_points = data[data['gmt_offset'].isna() | data['epa_region'].isna()]\n",
    "\n",
    "# # 创建 GeoDataFrame\n",
    "# known_gdf = pd.DataFrame({\n",
    "#     'geometry': [Point(lon, lat) for lon, lat in zip(known_points['Lon'], known_points['Lat'])],\n",
    "#     'gmt_offset': known_points['gmt_offset'],\n",
    "#     'epa_region': known_points['epa_region']\n",
    "# })\n",
    "# unknown_gdf = pd.DataFrame({\n",
    "#     'geometry': [Point(lon, lat) for lon, lat in zip(unknown_points['Lon'], unknown_points['Lat'])],\n",
    "#     'index': unknown_points.index\n",
    "# })\n",
    "\n",
    "# # 构建 cKDTree 用于快速查找最近邻\n",
    "# tree = cKDTree([(p.x, p.y) for p in known_gdf['geometry']])\n",
    "\n",
    "# # 为每个未知点找到最近的已知点\n",
    "# for _, row in unknown_gdf.iterrows():\n",
    "#     point = row['geometry']\n",
    "#     _, nearest_index = tree.query([point.x, point.y])\n",
    "#     nearest_point = known_gdf.iloc[nearest_index]\n",
    "#     # 直接使用值来填充，而不是访问 values 属性\n",
    "#     data.loc[row['index'], 'gmt_offset'] = nearest_point['gmt_offset']\n",
    "#     data.loc[row['index'], 'epa_region'] = nearest_point['epa_region']\n",
    "\n",
    "# # 将处理后的数据保存到新的 CSV 文件\n",
    "# output_file = 'output/Region/MonitorsTimeRegion_Filter_Add.csv'\n",
    "# data.to_csv(output_file, index=False)\n",
    "# print(f\"处理后的数据已保存到 {output_file}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "# import pandas as pd\n",
    "\n",
    "# # 读取CSV文件\n",
    "# df = pd.read_csv('output/Region/MonitorsTimeRegion_Filter.csv')\n",
    "\n",
    "# # 准备新的数据行，以字典形式提供，键为列名，值为对应的值\n",
    "# new_row = {\n",
    "#    'site_id': '80699991',  # 替换为实际的站点ID\n",
    "#     'Lat': 40.2778,  # 替换为实际的纬度值\n",
    "#     'Lon': -105.5453,  # 替换为实际的经度值\n",
    "#     'gmt_offset': -7.0,  # 替换为实际的 GMT 偏移值\n",
    "#     'epa_region': '8.0'  # 替换为实际的 EPA 区域\n",
    "# }\n",
    "\n",
    "# # 将新数据行添加到DataFrame中\n",
    "# df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "# # 将修改后的数据写回到CSV文件\n",
    "# df.to_csv('output/Region/MonitorsTimeRegion_Filter.csv', index=False) \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
